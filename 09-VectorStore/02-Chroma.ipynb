{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "635d8ebb",
      "metadata": {},
      "source": [
        "# Chroma With Langchain\n",
        "\n",
        "- Author: [Gwangwon Jung](https://github.com/pupba)\n",
        "- Design: []()\n",
        "- Peer Review: \n",
        "- This is a part of [LangChain Open Tutorial](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial)\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb) [![Open in GitHub](https://img.shields.io/badge/Open%20in%20GitHub-181717?style=flat-square&logo=github&logoColor=white)](https://github.com/LangChain-OpenTutorial/LangChain-OpenTutorial/blob/main/09-VectorStore/02-Chroma.ipynb)\n",
        "\n",
        "## Overview\n",
        "\n",
        "This tutorial covers how to use `Chroma Vector Store` with `LangChain` .\n",
        "\n",
        "`Chroma` is an `open-source AI application database` .\n",
        "\n",
        "In this tutorial, after learning how to use `langchain-chroma` , we will implement examples of a simple **Text Search** engine using `Chroma` .\n",
        "\n",
        "![search-example](./assets/02-chroma-with-langchain-flow-search-example.png)\n",
        "\n",
        "### Table of Contents\n",
        "\n",
        "- [Overview](#overview)\n",
        "- [Environement Setup](#environment-setup)\n",
        "- [What is Chroma?](#what-is-chroma?)\n",
        "- [LangChain Chroma Basic](#langchain-chroma-basic)\n",
        "- [Manage Store](#manage-store)\n",
        "- [Query Vector Store](#query-vector-store)\n",
        "\n",
        "\n",
        "### References\n",
        "\n",
        "- [Chroma Docs](https://docs.trychroma.com/docs/overview/introduction)\n",
        "- [Langchain-Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma/)\n",
        "- [List of VectorStore supported by Langchain](https://python.langchain.com/docs/integrations/vectorstores/)\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6c7aba4",
      "metadata": {},
      "source": [
        "## Environment Setup\n",
        "\n",
        "Set up the environment. You may refer to [Environment Setup](https://wikidocs.net/257836) for more details.\n",
        "\n",
        "**[Note]**\n",
        "- `langchain-opentutorial` is a package that provides a set of easy-to-use environment setup, useful functions and utilities for tutorials. \n",
        "- You can checkout the [`langchain-opentutorial`](https://github.com/LangChain-OpenTutorial/langchain-opentutorial-pypi) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "21943adb",
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain-opentutorial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "f25ec196",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "from langchain_opentutorial import package\n",
        "\n",
        "package.install(\n",
        "    [\n",
        "        \"langsmith\",\n",
        "        \"langchain-core\",\n",
        "        \"langchain-chroma\",\n",
        "        \"chromadb\",\n",
        "        \"langchain-text-splitters\",\n",
        "        \"langchain-huggingface\",\n",
        "        \"python-dotenv\",\n",
        "    ],\n",
        "    verbose=False,\n",
        "    upgrade=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "7f9065ea",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Environment variables have been set successfully.\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables\n",
        "from langchain_opentutorial import set_env\n",
        "\n",
        "set_env(\n",
        "    {\n",
        "        \"OPENAI_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_API_KEY\": \"\",\n",
        "        \"LANGCHAIN_TRACING_V2\": \"true\",\n",
        "        \"LANGCHAIN_ENDPOINT\": \"https://api.smith.langchain.com\",\n",
        "        \"LANGCHAIN_PROJECT\": \"Chroma\",\n",
        "        \"HUGGINGFACEHUB_API_TOKEN\": \"\",\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690a9ae0",
      "metadata": {},
      "source": [
        "You can alternatively set API keys such as `OPENAI_API_KEY` in a `.env` file and load them.\n",
        "\n",
        "[Note] This is not necessary if you've already set the required API keys in previous steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f99b5b6",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load API keys from .env file\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77dbe5de",
      "metadata": {},
      "source": [
        "## What is Chroma?\n",
        "\n",
        "![logo](./assets/02-chroma-with-langchain-chroma-logo.png)\n",
        "\n",
        "`Chroma` is the `open-source vector database` designed for AI application. \n",
        "\n",
        "It specializes in storing high-dimensional vectors and performing fast similariy search, making it ideal for tasks like `semantic search` , `recommendation systems` and `multimodal search` .\n",
        "\n",
        "With its **developer-friendly APIs** and seamless integration with frameworks like `LangChain` , `Chroma` is powerful tool for building scalable, AI-driven solutions.\n",
        "\n",
        "The biggest feature of `Chroma` is that it internally **Indexing ([HNSW](https://en.wikipedia.org/wiki/Hierarchical_navigable_small_world))** and **Embedding ([all-MiniLM-L6-v2](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2))** are used when storing data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb6d8430",
      "metadata": {},
      "source": [
        "## LangChain Chroma Basic"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13874284",
      "metadata": {},
      "source": [
        "### Create VectorDB\n",
        "\n",
        "The **library** supported by `LangChain` has no `upsert` function and lacks interface uniformity with other **Vector DBs**, so we have implemented a new **Python** class.\n",
        "\n",
        "First, Defines a **Python** class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "7b799511",
      "metadata": {},
      "outputs": [],
      "source": [
        "from utils.vectordbinterface import VectorDBInterface\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_chroma.vectorstores import cosine_similarity\n",
        "from langchain_core.vectorstores.base import VectorStoreRetriever\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any, Optional, Union, Tuple\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from uuid import uuid4\n",
        "\n",
        "\n",
        "class ChromaDB(VectorDBInterface):\n",
        "    def __init__(self, embeddings: Optional[Any] = None):\n",
        "        self.chroma = None\n",
        "        self.unique_ids = set()\n",
        "        self._embeddings = embeddings if embeddings is not None else None\n",
        "        self._embeddings_function = (\n",
        "            embeddings.embed_documents\n",
        "            if embeddings is not None\n",
        "            else embedding_functions.DefaultEmbeddingFunction()  # all-MiniLM-L6v2\n",
        "        )\n",
        "        self.chroma_search = None\n",
        "\n",
        "    def connect(self, **kwargs) -> None:\n",
        "        \"\"\"\n",
        "        ChromaDB Connect\n",
        "        \"\"\"\n",
        "        langchain_config = {}\n",
        "\n",
        "        if kwargs[\"mode\"] == \"in-memory\":  # In-Memory\n",
        "            chroma_client = chromadb.Client()\n",
        "\n",
        "        elif kwargs[\"mode\"] == \"persistent\":  # Local\n",
        "            chroma_client = chromadb.PersistentClient(path=kwargs[\"persistent_path\"])\n",
        "            langchain_config[\"persist_directory\"] = kwargs[\"persistent_path\"]\n",
        "\n",
        "        elif kwargs[\"mode\"] == \"server\":  # Server-Client\n",
        "            chroma_client = chromadb.HttpClient(\n",
        "                host=kwargs[\"host\"], port=kwargs[\"port\"]\n",
        "            )\n",
        "        else:\n",
        "            raise Exception(\n",
        "                \"Invalid Input, Enter one of ['in-meory','persistent','server'] modes.\"\n",
        "            )\n",
        "\n",
        "        # The Chroma client allows you to get and delete existing collections by their name.\n",
        "        # It also offers a get or create method to get a collection if it exists, or create it otherwise.\n",
        "\n",
        "        # l2(default) : squared L2 norm\n",
        "        # ip : Inner Product\n",
        "        # cosine : Cosine Distance\n",
        "        metadata = {\n",
        "            \"hnsw:space\": (\n",
        "                kwargs.get(\"hnsw:space\") if kwargs.get(\"hnsw:space\", None) else \"l2\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        self.chroma = chroma_client.get_or_create_collection(\n",
        "            name=kwargs[\"collection\"], metadata=metadata\n",
        "        )\n",
        "\n",
        "        langchain_config[\"collection_name\"] = kwargs[\"collection\"]\n",
        "        langchain_config[\"collection_metadata\"] = metadata\n",
        "\n",
        "        existing_ids = self.chroma.get(include=[])[\"ids\"]  # Get existing unique ids\n",
        "        self.unique_ids.update(existing_ids)  # current unique ids update\n",
        "\n",
        "        # Langchain-chroma for Search\n",
        "        self.chroma_search = Chroma(\n",
        "            **langchain_config,\n",
        "            embedding_function=self._embeddings,\n",
        "        )\n",
        "\n",
        "    def create_index(\n",
        "        self, index_name: str, dimension: int, metric: str = \"dotproduct\", **kwargs\n",
        "    ) -> Any:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def get_index(self, index_name: str) -> Any:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def delete_index(self, index_name: str) -> None:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def list_indexs(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None\n",
        "\n",
        "    def add(self, pre_documents: List[Document], **kwargs) -> None:\n",
        "        documents = []\n",
        "        metadatas = []\n",
        "        ids = []\n",
        "        for doc in pre_documents:\n",
        "            documents.append(doc.page_content)\n",
        "            ids.append(doc.metadata[\"id\"])\n",
        "            metadatas.append(\n",
        "                {key: value for key, value in doc.metadata.items() if key != \"id\"}\n",
        "            )\n",
        "\n",
        "        embeddings = self._embeddings_function(documents)  # embedding documents\n",
        "\n",
        "        self.chroma.add(\n",
        "            documents=documents,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=metadatas,\n",
        "            ids=ids,\n",
        "        )\n",
        "        self.unique_ids.update(ids)\n",
        "\n",
        "    def upsert_documents(\n",
        "        self,\n",
        "        documents: List[Dict],\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Upsert documents to Chroma\n",
        "\n",
        "        :param documents: List of documents\n",
        "        :param embedding_function: Embedding function\n",
        "        \"\"\"\n",
        "        # Embedding documents\n",
        "        embeddings = self._embeddings_function([doc.page_content for doc in documents])\n",
        "        # Generate unique ids\n",
        "        unique_ids = [doc.metadata[\"id\"] for doc in documents]\n",
        "        # Upsert documents\n",
        "        self.chroma.upsert(\n",
        "            ids=unique_ids,\n",
        "            embeddings=embeddings,\n",
        "            metadatas=[doc.metadata for doc in documents],\n",
        "            documents=[doc.page_content for doc in documents],\n",
        "        )\n",
        "\n",
        "        print(\"Success Upsert All Documents\")\n",
        "\n",
        "        # update unique_ids\n",
        "        self.unique_ids.update(unique_ids)\n",
        "\n",
        "    def upsert_documents_parallel(\n",
        "        self,\n",
        "        documents: List[Dict],\n",
        "        batch_size: int = 32,\n",
        "        max_workers: int = 10,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Parallel upsert documents to Chroma\n",
        "        :param documents: List of documents\n",
        "        :param batch_size: Batch size\n",
        "        :param max_workers: Number of workers\n",
        "        \"\"\"\n",
        "        # split documents into batches\n",
        "        batches = [\n",
        "            documents[i : i + batch_size] for i in range(0, len(documents), batch_size)\n",
        "        ]\n",
        "        all_unique_ids = set()  # Store all unique IDs from all batches\n",
        "        failed_uids = []  # Store failed batches\n",
        "\n",
        "        # Parallel processing\n",
        "        with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "            futures = [\n",
        "                executor.submit(self.upsert_documents, batch, **kwargs)\n",
        "                for batch in batches\n",
        "            ]\n",
        "\n",
        "        # Wait for all futures to complete\n",
        "        for future, batch in zip(as_completed(futures), batches):\n",
        "            try:\n",
        "                future.result()  # Wait for the batch to complete\n",
        "                # Extract unique IDs from the batch\n",
        "                unique_ids = [doc.metadata[\"id\"] for doc in batch]\n",
        "                all_unique_ids.update(unique_ids)  # Add to the total set\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred during upsert: {e}\")\n",
        "                failed_uids.append(unique_ids)  # Store failed batch for retry\n",
        "\n",
        "        self.unique_ids.update(all_unique_ids)\n",
        "\n",
        "        print(f\"Success Upsert Parallel All Documents\\nFailed Batches: {failed_uids}\")\n",
        "\n",
        "    def _cosine_similarity_search_text(\n",
        "        self, query: str, configs: Dict\n",
        "    ) -> List[Tuple[Document, float]]:\n",
        "        \"\"\"\n",
        "        Hybrid Search : Text Search + Cosine Similarity\n",
        "        \"\"\"\n",
        "        docs = self.chroma_search.similarity_search(**configs)\n",
        "\n",
        "        embx = self._embeddings.embed_query(query)\n",
        "        emb_d = self._embeddings.embed_documents([doc.page_content for doc in docs])\n",
        "\n",
        "        scores = cosine_similarity([embx], emb_d)\n",
        "\n",
        "        return sorted(\n",
        "            [(doc, score) for score, doc in zip(scores[0], docs)],\n",
        "            key=lambda x: x[1],\n",
        "            reverse=True,\n",
        "        )\n",
        "\n",
        "    def query(\n",
        "        self,\n",
        "        query: str,\n",
        "        top_k: int = 10,\n",
        "        score: bool = False,\n",
        "        filters: Optional[Dict[str, str]] = None,\n",
        "        where_document: Optional[Dict[str, str]] = None,\n",
        "        cs: bool = False,\n",
        "        **kwargs,\n",
        "    ) -> Union[List[Document], List[Tuple[Document, float]]]:\n",
        "        \"\"\"\n",
        "        A Method that implements a search method using a LangChain-Chroma library.\n",
        "        \"\"\"\n",
        "\n",
        "        configs = {\n",
        "            \"query\": query,\n",
        "            \"k\": top_k,\n",
        "            \"filter\": filters,\n",
        "            **kwargs,\n",
        "        }\n",
        "\n",
        "        if score:\n",
        "            configs[\"where_document\"] = where_document\n",
        "            results = self.chroma_search.similarity_search_with_score(\n",
        "                **configs\n",
        "            )  # distance search score\n",
        "\n",
        "        elif cs:  # cosine similarity search\n",
        "            return self._cosine_similarity_search_text(query, configs)\n",
        "        else:\n",
        "            results = self.chroma_search.similarity_search(**configs)\n",
        "\n",
        "        return results\n",
        "\n",
        "    def delete_by_filter(\n",
        "        self, unique_ids: List[str], filters: Optional[Dict] = None, **kwargs\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Delete documents by filter\n",
        "        :param unique_ids: List of unique ids\n",
        "        :param filters: Filter conditions\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.chroma.delete(\n",
        "                ids=unique_ids,\n",
        "                where=filters,\n",
        "            )\n",
        "            pre_count = len(self.unique_ids)\n",
        "            self.unique_ids = set(self.chroma.get(include=[])[\"ids\"])\n",
        "\n",
        "            print(f\"Success Delete {pre_count-len(self.unique_ids)} Documents\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    def getRetriever(\n",
        "        self, search_type: str = \"mmr\", search_kwargs: Optional[Dict] = None\n",
        "    ) -> VectorStoreRetriever:\n",
        "        \"\"\"\n",
        "        Get Retriever Method using a LangChain-Chroma library.\n",
        "\n",
        "        Refer to the following document -> LangChain-Chroma Official Document.\n",
        "\n",
        "        :param search_type: [similarity(default), mmr, similarity_score_threshold]\n",
        "        :param search_kwargs: [k, fetch_k, lambda_mult, filter]\n",
        "        \"\"\"\n",
        "        return self.chroma_search.as_retriever(\n",
        "            search_type=search_type, search_kwargs=search_kwargs\n",
        "        )\n",
        "\n",
        "    def preprocess_documents(\n",
        "        self,\n",
        "        documents: List[Document],\n",
        "        source: Optional[str] = None,\n",
        "        author: Optional[str] = None,\n",
        "        chapter: bool = False,\n",
        "        **kwargs,\n",
        "    ) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Change LangChain Document to Chroma\n",
        "\n",
        "        Refer to the following document -> LangChain-Chroma Official Document.\n",
        "\n",
        "        :param documents: List of LangChain documents\n",
        "        :param source: Source of the document\n",
        "        :param author: Author of the document\n",
        "        :param chapter: Chapter of the document\n",
        "        :return: List of Chroma documents\n",
        "        \"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        if source is not None:\n",
        "            metadata[\"source\"] = source\n",
        "        if author is not None:\n",
        "            metadata[\"author\"] = author\n",
        "\n",
        "        processed_docs = []\n",
        "        current_chapter = None\n",
        "        save_flag = False\n",
        "        for doc in documents:\n",
        "            content = doc.page_content\n",
        "\n",
        "            content = content.replace(\"(picture)\\n\", \"\")\n",
        "\n",
        "            # Chapter dectect\n",
        "            if content.startswith(\"[ Chapter \") and \"\\n\" in content:\n",
        "                # Chapter Num (example: \"[ Chapter 26 ]\\n\" -> 26)\n",
        "                chapter_part, content_part = content.split(\"\\n\", 1)\n",
        "                current_chapter = int(chapter_part.split()[2].strip(\"]\"))\n",
        "                content = content_part\n",
        "\n",
        "            elif content.strip() == \"[ END ]\":\n",
        "                break\n",
        "\n",
        "            if current_chapter is not None:\n",
        "                # add metadata\n",
        "                if chapter:\n",
        "                    metadata[\"chapter\"] = current_chapter\n",
        "                updated_metadata = {**doc.metadata, **metadata, \"id\": str(uuid4())}\n",
        "                # Document append to processed_docs\n",
        "                processed_docs.append(\n",
        "                    Document(metadata=updated_metadata, page_content=content)\n",
        "                )\n",
        "\n",
        "        return processed_docs\n",
        "\n",
        "    def get_api_key(self) -> str:\n",
        "        \"\"\"\n",
        "        Not used in Chroma\n",
        "        \"\"\"\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6f20414",
      "metadata": {},
      "source": [
        "### Select Embedding Model\n",
        "\n",
        "We load the **Embedding Model** with `langchain_huggingface` .\n",
        "\n",
        "If you want to use a different model, use a different model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fead7517",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"Alibaba-NLP/gte-base-en-v1.5\"\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name, model_kwargs={\"trust_remote_code\": True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b4e5e6d",
      "metadata": {},
      "source": [
        "Create `ChromaDB` object.\n",
        "\n",
        "- **Mode** : `persistent`\n",
        "\n",
        "- **Persistent Path** : `data/chroma.sqlite` (Used `SQLite` DB)\n",
        "\n",
        "- **collection** : `test`\n",
        "\n",
        "- **hnsw:space** : `cosine`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "27ccfa77",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store = ChromaDB(embeddings=embeddings)\n",
        "\n",
        "configs = {\n",
        "    \"mode\": \"persistent\",\n",
        "    \"persistent_path\": \"data/chroma_text\",\n",
        "    \"collection\": \"test\",\n",
        "    \"hnsw:space\": \"cosine\",\n",
        "}\n",
        "\n",
        "vector_store.connect(**configs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "492fd4a1",
      "metadata": {},
      "source": [
        "### Load Text Documents Data\n",
        "\n",
        "In this tutorial, we will use the `A Little Prince` fairy tale document.\n",
        "\n",
        "To put this data in `Chroma` ,we will do data preprocessing first.\n",
        "\n",
        "First of all, we will load the `data/the_little_prince.txt` file that extracted only the text of the fairy tale document.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8dd04292",
      "metadata": {},
      "outputs": [],
      "source": [
        "# If your \"OS\" is \"Windows\", add 'encoding=utf-8' to the open function\n",
        "with open(\"./data/the_little_prince.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1534b127",
      "metadata": {},
      "source": [
        "Second, chunking the text imported into the `RecursiveCharacterTextSplitter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3b4b30b8",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Content: The Little Prince\n",
            "Written By Antoine de Saiot-Exupery (1900〜1944)\n",
            "Metadata: {}\n",
            "\n",
            "Content: [ Antoine de Saiot-Exupery ]\n",
            "Metadata: {}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    # Set a really small chunk size, just to show.\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len,\n",
        "    is_separator_regex=False,\n",
        ")\n",
        "\n",
        "split_docs = text_splitter.create_documents([raw_text])\n",
        "\n",
        "for docs in split_docs[:2]:\n",
        "    print(f\"Content: {docs.page_content}\\nMetadata: {docs.metadata}\", end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "558e5fe7",
      "metadata": {},
      "source": [
        "Preprocessing document for `Chroma` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ed0952fb",
      "metadata": {},
      "outputs": [],
      "source": [
        "pre_dosc = vector_store.preprocess_documents(\n",
        "    documents=split_docs,\n",
        "    source=\"The Little Prince\",\n",
        "    author=\"Antoine de Saint-Exupéry\",\n",
        "    chapter=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7d92d3bc",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': '6af01607-86b6-4a28-914f-1d0cb664da30'}, page_content='- we are introduced to the narrator, a pilot, and his ideas about grown-ups'),\n",
              " Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': '7e4d92d3-ce7d-4a5f-9776-9dfb5155960f'}, page_content='Once when I was six years old I saw a magnificent picture in a book, called True Stories from')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b68dfd9a",
      "metadata": {},
      "source": [
        "## Manage Store\n",
        "\n",
        "This section introduces four basic functions.\n",
        "\n",
        "- `add`\n",
        "\n",
        "- `upsert(parallel)`\n",
        "\n",
        "- `query`\n",
        "\n",
        "- `delete`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a80a3015",
      "metadata": {},
      "source": [
        "### Add\n",
        "\n",
        "Add the new `Documents` .\n",
        "\n",
        "An error occurs if you have the same `ID` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "725bbe68",
      "metadata": {},
      "outputs": [],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f800cb49",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['6af01607-86b6-4a28-914f-1d0cb664da30',\n",
              " '7e4d92d3-ce7d-4a5f-9776-9dfb5155960f']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "uids = list(vector_store.unique_ids)\n",
        "uids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "68e78f78",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['6af01607-86b6-4a28-914f-1d0cb664da30'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['- we are introduced to the narrator, a pilot, and his ideas about grown-ups'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.chroma.get(ids=uids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cb0babb",
      "metadata": {},
      "source": [
        "Error occurs when trying to `add` duplicate `ids` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3d57fac6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Insert of existing embedding ID: 6af01607-86b6-4a28-914f-1d0cb664da30\n",
            "Insert of existing embedding ID: 7e4d92d3-ce7d-4a5f-9776-9dfb5155960f\n",
            "Add of existing embedding ID: 6af01607-86b6-4a28-914f-1d0cb664da30\n",
            "Add of existing embedding ID: 7e4d92d3-ce7d-4a5f-9776-9dfb5155960f\n"
          ]
        }
      ],
      "source": [
        "vector_store.add(pre_documents=pre_dosc[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c63a62b3",
      "metadata": {},
      "source": [
        "### Upsert(parallel)\n",
        "\n",
        "`Upsert` will `Update` a document or `Add` a new document if the same `ID` exists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 195,
      "id": "4a9ca529",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'ids': ['fcaeced0-f01e-4326-9603-0c0bd4c72d40',\n",
              "  '0f3c8748-85f9-436b-afa3-9eba06c90d96'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['- we are introduced to the narrator, a pilot, and his ideas about grown-ups',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 195,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 196,
      "id": "8edc27d3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'The Little Prince', 'author': 'Antoine de Saint-Exupéry', 'chapter': 1, 'id': 'fcaeced0-f01e-4326-9603-0c0bd4c72d40'}, page_content='Changed Content')"
            ]
          },
          "execution_count": 196,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pre_dosc[0].page_content = \"Changed Content\"\n",
        "pre_dosc[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 197,
      "id": "d0086c07",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Upsert All Documents\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'ids': ['fcaeced0-f01e-4326-9603-0c0bd4c72d40',\n",
              "  '0f3c8748-85f9-436b-afa3-9eba06c90d96'],\n",
              " 'embeddings': None,\n",
              " 'documents': ['Changed Content',\n",
              "  'Once when I was six years old I saw a magnificent picture in a book, called True Stories from'],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [{'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': 'fcaeced0-f01e-4326-9603-0c0bd4c72d40',\n",
              "   'source': 'The Little Prince'},\n",
              "  {'author': 'Antoine de Saint-Exupéry',\n",
              "   'chapter': 1,\n",
              "   'id': '0f3c8748-85f9-436b-afa3-9eba06c90d96',\n",
              "   'source': 'The Little Prince'}],\n",
              " 'included': [<IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "execution_count": 197,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vector_store.upsert_documents(\n",
        "    documents=pre_dosc[:2],\n",
        ")\n",
        "tmp_ids = [docs.metadata[\"id\"] for docs in pre_dosc[:2]]\n",
        "vector_store.chroma.get(ids=tmp_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d743a327",
      "metadata": {},
      "outputs": [],
      "source": [
        "# parallel upsert\n",
        "vector_store.upsert_documents_parallel(\n",
        "    documents=pre_dosc,\n",
        "    batch_size=32,\n",
        "    max_workers=10,\n",
        ")\n",
        "# Clear Output cell, because it is too long."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d515ad98",
      "metadata": {},
      "source": [
        "## Query Vector Store\n",
        "\n",
        "There are two ways to `Query` the `LangChain Chroma Vector Store` .\n",
        "\n",
        "- **Directly** : Query the vector store directly using methods like `similarity_search` or `similarity_search_with_score` .\n",
        "\n",
        "- **Turning into retriever** : Convert the vector store into a `retriever` object, which can be used in `LangChain` pipelines or chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d37c37fb",
      "metadata": {},
      "source": [
        "### Query\n",
        "\n",
        "This method is created by wrapping the methods of the `langchain-chroma` .\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `query:str` - Query text to search for.\n",
        "\n",
        "- `k:int = DEFAULT_K` - Number of results to return. Defaults to 4.\n",
        "\n",
        "- `filter: Dict[str, str] | None = None` - Filter by metadata. Defaults to None.\n",
        "\n",
        "- `where_document: Dict[str, str] | None = None` - dict used to filter by the documents. E.g. {$contains: {\"text\": \"hello\"}}.\n",
        "\n",
        "- `**kwargs:Any` : Additional keyword arguments to pass to Chroma collection query.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "- `List[Document]` - List of documents most similar to the query text and distance in float for each. Lower score represents more similarity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99da0a7b",
      "metadata": {},
      "source": [
        "**Simple Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "b77b3f7e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 5d929785-71e7-4272-be7c-da48e09c6fed\n",
            "Chapter: 7\n",
            "Page Content: prince disturbed my thoughts.\n",
            "\n",
            "ID: 00ea7e38-5675-48a8-ad41-d426595b370c\n",
            "Chapter: 6\n",
            "Page Content: Oh, little prince! Bit by bit I came to understand the secrets of your sad little life... For a\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vector_store.query(query=\"Prince\", top_k=2)\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.id)\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03207993",
      "metadata": {},
      "source": [
        "**Filtering Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "a63c9365",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 0e133cbf-5eac-4045-a664-3df01e544b39\n",
            "Chapter: 20\n",
            "Page Content: snow, the little prince at last came upon a road. And all roads lead to the abodes of men.\n",
            "\n",
            "ID: 526197f8-fd2f-4f02-9fba-44b2e79b6371\n",
            "Chapter: 20\n",
            "Page Content: extinct forever... that doesn‘t make me a very great prince...\"\n",
            "\n"
          ]
        }
      ],
      "source": [
        "docs = vector_store.query(query=\"Prince\", top_k=2, filters={\"chapter\": 20})\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.id)\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9ad001e",
      "metadata": {},
      "source": [
        "**Cosine Similarity Search**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "04e592b0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 0e133cbf-5eac-4045-a664-3df01e544b39\n",
            "Chapter: 20\n",
            "Page Content: snow, the little prince at last came upon a road. And all roads lead to the abodes of men.\n",
            "Similarity Score: 60.0%\n",
            "\n",
            "ID: 526197f8-fd2f-4f02-9fba-44b2e79b6371\n",
            "Chapter: 20\n",
            "Page Content: extinct forever... that doesn‘t make me a very great prince...\"\n",
            "Similarity Score: 54.0%\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Cosine Similarity\n",
        "\n",
        "results = vector_store.query(query=\"Prince\", top_k=2, cs=True, filters={\"chapter\": 20})\n",
        "\n",
        "for doc, score in results:\n",
        "    print(\"ID:\", doc.id)\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print(f\"Similarity Score: {round(score,2)*100:.1f}%\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3de5b67",
      "metadata": {},
      "source": [
        "### getRetriever()\n",
        "\n",
        "The `getRetriever()` method is wrapping a `as_retriever()` . \n",
        "\n",
        "The `as_retriever()` method converts a `VectorStore` object into a `Retriever` object.\n",
        "\n",
        "A `Retriever` is an interface used in `LangChain` to query a vector store and retrieve relevant documents.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `search_type:Optional[str]` - Defines the type of search that the Retriever should perform. Can be `similarity` (default), `mmr` , or `similarity_score_threshold`\n",
        "\n",
        "- `search_kwargs:Optional[Dict]` - Keyword arguments to pass to the search function. \n",
        "\n",
        "    Can include things like:\n",
        "\n",
        "    `k` : Amount of documents to return (Default: 4)\n",
        "\n",
        "    `score_threshold` : Minimum relevance threshold for similarity_score_threshold\n",
        "\n",
        "    `fetch_k` : Amount of documents to pass to `MMR` algorithm(Default: 20)\n",
        "        \n",
        "    `lambda_mult` : Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum. (Default: 0.5)\n",
        "\n",
        "    `filter` : Filter by document metadata\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `VectorStoreRetriever` - Retriever class for VectorStore.\n",
        "\n",
        "\n",
        "### invoke()\n",
        "\n",
        "Invoke the retriever to get relevant documents.\n",
        "\n",
        "Main entry point for synchronous retriever invocations.\n",
        "\n",
        "**Parameters**\n",
        "\n",
        "- `input:str` - The query string.\n",
        "- `config:RunnableConfig | None = None` - Configuration for the retriever. Defaults to None.\n",
        "- `**kwargs:Any` - Additional arguments to pass to the retriever.\n",
        "\n",
        "\n",
        "**Returns**\n",
        "\n",
        "- `List[Document]` : List of relevant documents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "600b208a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ID: 236147a8-8657-4d52-ab3a-9b52037bba44\n",
            "Chapter: 5\n",
            "Page Content: Indeed, as I learned, there were on the planet where the little prince lived-- as on all planets--\n",
            "\n",
            "ID: 5c9975bc-8214-45f9-85d0-a2489822df82\n",
            "Chapter: 5\n",
            "Page Content: Now there were some terrible seeds on the planet that was the home of the little prince; and these\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retriever = vector_store.getRetriever(search_type=\"similarity\", search_kwargs={\"k\": 2})\n",
        "docs = retriever.invoke(\"Prince\", filter={\"chapter\": 5})\n",
        "\n",
        "for doc in docs:\n",
        "    print(\"ID:\", doc.id)\n",
        "    print(\"Chapter:\", doc.metadata[\"chapter\"])\n",
        "    print(\"Page Content:\", doc.page_content)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "388f5512",
      "metadata": {},
      "source": [
        "### Delete\n",
        "\n",
        "`Delete` the Documents.\n",
        "\n",
        "You can use with `filter` ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "id": "a3c4af1e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1327"
            ]
          },
          "execution_count": 263,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "id": "6412f23b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "43"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len([docs for docs in pre_dosc if docs.metadata[\"chapter\"] == 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "id": "a3c99973",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 43 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(\n",
        "    unique_ids=list(vector_store.unique_ids), filters={\"chapter\": 1}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "id": "7349a83b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1284"
            ]
          },
          "execution_count": 266,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "id": "70991cfe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Success Delete 1284 Documents\n"
          ]
        }
      ],
      "source": [
        "vector_store.delete_by_filter(unique_ids=list(vector_store.unique_ids))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "id": "b53c861e",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 268,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(vector_store.unique_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05f53e7c",
      "metadata": {},
      "source": [
        "Remove a `Huggingface Cache` and `ChromaDB Object`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1ffecd99",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DeleteCacheStrategy(expected_freed_size=0, blobs=frozenset(), refs=frozenset(), repos=frozenset(), snapshots=frozenset())"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import scan_cache_dir\n",
        "\n",
        "del embeddings\n",
        "del vector_store\n",
        "scan = scan_cache_dir()\n",
        "scan.delete_revisions()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain-opentutorial-B290FrwJ-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
